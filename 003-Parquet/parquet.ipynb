{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Context**\n",
    "\n",
    "`Source`: Demystifying the Parquet File Format On towardsdatascience.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Definition: OLAP (Online Analytical Processing)**\n",
    "\n",
    "`Source`: orcale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a spreadsheet, data is presented two-dimensionally in rows and columns. For each item of data, both the row and the column must always have an entry, even if this means that the same date, the same product or the same customer is listed many times. This results in very large spreadsheets that quickly become cumbersome and awkward to use — all the more so when more than one person has access and makes changes to them.\n",
    "\n",
    "Spreadsheets are similar to SQL and relational databases, as well as to traditional data warehouses, which all store data in rows and columns in a two-dimensional format and suffer from some of the same shortcomings. Pulling data from a very large relational database, for instance, can be a slow process and reorganizing the results to answer different questions can be a labor-intensive chore.\n",
    "\n",
    "OLAP allows data to be stored in three or more dimensions instead of just two, which leads to a number of advantages. Presented in the form of a cube, the same data point (like a date or a SKU code) needs to be entered only once, making for faster searches and easier extraction. Also, cubes can be sliced, diced and rotated in a number of ways, allowing the user to narrow or broaden a search and take a variety of approaches to visualizing the data.\n",
    "\n",
    "This makes OLAP a powerful tool for data discovery and what-if forecasting, and is the reason it is used as the underlying technology for many business intelligence (BI) applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **What Is an OLAP Cube?**\n",
    "\n",
    "The OLAP cube's data structure is optimized for very rapid data analysis. It includes numeric facts called measures, which are organized along a three-way axis.\n",
    "\n",
    "For example, a company might organize its sales data by product, timeframe and location for comparative purposes. In this case, the OLAP cube's three dimensions might be product, month and store. Later, more layers can be added, creating additional dimensions. The top layer of the cube might organize sales by store, for instance, but additional layers could be added for city, state and country. Multidimensional OLAP databases with more than three dimensions are known as hypercubes.\n",
    "\n",
    "Smaller cubes can also be carved out from the larger parent cube. The store layer, for example, could contain cubes arranged by product, month and salesperson.\n",
    "\n",
    "<img src=\"./images/olap_cube.avif\" width=\"500px\" style=\"background-color:white;padding:20px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **OLAP Explained**\n",
    "\n",
    "Businesses use many factors to track their activities. When these are tracked on a spreadsheet or a relational database management system (RDMS), they are logically grouped into two dimensions on the 'x' and the 'y' axes.\n",
    "\n",
    "Monthly sales, for example, may be tracked by displaying products in a column on the y axis and the months of the year in a row on the x axis. Each x and y intersection would hold the entry for the amount of sales of a particular product in a given month. Realistically, however, a business may also want to track its sales by location, salesperson, discounts applied and any number of additional factors. Managers need to do this in order to drill down into particular questions (\"Why did this product stop selling in Chicago in July?\") and to get a more complete picture of how the business is faring (\"Why was revenue flat even though sales were up for the year? Was too much discounting taking place?\").\n",
    "\n",
    "Using OLAP, new dimensions can be added to an OLAP data cube to track each of these variables. Then analysts can take any view — any slice, section or angle of the cube — to produce a report that contains the key points of interest. Monthly product sales in San Francisco? No problem. Compare sales in months with more discounting to months with less discounting? Also done in a snap.\n",
    "\n",
    "Better still, any of this data can be presented visually, making it much easier to identify trends. And the view can be changed by simply rotating the cube. So, a chart that shows which products sold best in New York in November can be turned to show in which month products sales were best in New York.\n",
    "\n",
    "All this can be done very quickly and with a minimum of fuss, giving a business the answers it needs both for long-term planning and to monitor its day-to-day operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **How Does OLAP Work?**\n",
    "\n",
    "OLAP performs rapid, in-depth analysis on large volumes of data. The data comes from one or more data warehouses, data marts or some other type of centralized data store. The data can also come from different types of sources, including spreadsheets, emails and text documents, as well as audio and video files.\n",
    "\n",
    "Once it is extracted, the data is stored in a data warehouse (which could be the same one it came from in the first place), where it is cleansed and formatted into OLAP cubes. The cubes are then loaded onto an OLAP server and some initial calculations take place, which ready the data for further analysis. Using an OLAP client, an analyst or business user can now pull the data from the OLAP cubes by running queries against it.\n",
    "\n",
    "<img src=\"./images/olap_process.avif\" width=\"700px\" style=\"background-color:white;padding:20px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Types of Analytical Operations OLAP Can Perform**\n",
    "\n",
    "OLAP cubes support four basic types of data analysis:\n",
    "\n",
    "1. **Drill-downs** provide a closer look and reveal more particulars about the data. This allows an analyst to extract details about individual products, for example, such as how well they are selling in stores versus online, or in one part of the country versus another.\n",
    "\n",
    "2. **Roll-ups** are the opposite of drill-downs, allowing the analyst to pull back and see the forest, not only the trees. For instance, instead of viewing sales data store by store, or salesperson by salesperson, a roll-up operation allows it to be viewed city by city or region by region.\n",
    "\n",
    "3. **Slicing and dicing** a smaller “sub-cube” out of a larger OLAP cube is another way to parse the data. By slicing the data by time period, for instance, an analyst might highlight all sales data for the first quarter. The analyst then might dice it to get a look at first quarter sales data in the New England region.\n",
    "\n",
    "4. **Pivoting or rotating** turns an OLAP cube so that one view of the data is replaced by another. So, for example, instead of comparing store sales by month, the analyst can rotate the data to compare monthly sales by store. This is quite similar to creating a pivot table in a spreadsheet but is much easier to do and requires less training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Row-based data storage**\n",
    "\n",
    "<img src=\"./images/row_based.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Column-based data storage**\n",
    "\n",
    "In this case, each column is a separate entity – meaning, each column is physically separated from other columns! Going back to our previous business question: the engine can now scan only those columns that are needed by the query, while skipping scanning the unnecessary columns. And, in most cases, this should improve the performance of the analytical queries.\n",
    "\n",
    "<img src=\"./images/column_based.png\" width=\"400px\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Parquet data storage**\n",
    "\n",
    "In OLAP scenarios, we are mainly concerned with two concepts: **projection** and **predicate(s)**. **`Projection`** refers to a **SELECT** statement in SQL language – which columns are needed by the query. Back to our previous questions (in row-based storage section), we need only the Product and Country columns, so the engine can skip scanning the remaining ones.\n",
    "\n",
    "**`Predicate(s)`** refer to the **WHERE** clause in SQL language – which rows satisfy criteria defined in the query. In our case, we are interested in T-Shirts only, so the engine can completely skip scanning Row group 2, where all the values in the Product column equal socks!\n",
    "\n",
    "<img src=\"./images/parquet.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Link between OLAP environments and Parquet format**\n",
    "\n",
    "Parquet is a column file format, which means it stores data by column rather than by row. This storage structure is particularly suited to the analysis and aggregation operations commonly performed in OLAP environments. By storing data by column, Parquet enables OLAP systems to access more efficiently the specific columns required for a given query, thus reducing the amount of data to be read from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Parquet**\n",
    "\n",
    "`Parquet` is a columnar storage format that is optimized for distributed processing of large datasets. It is widely used in Big Data processing systems like `Hadoop` and `Apache Spark`. \n",
    "\n",
    "A partitioned parquet file is a parquet file that is partitioned into multiple smaller files based on the values of one or more columns. Partitioning can significantly improve query performance by allowing the processing system to read only the necessary files.\n",
    "\n",
    "**Concepts**\n",
    "\n",
    "* **Parquet format**: A columnar storage format that is optimized for distributed processing of large datasets.\n",
    "\n",
    "* **Partitioning**: Dividing a dataset into smaller parts based on the values of one or more columns.\n",
    "\n",
    "* **Pandas DataFrame**: A two-dimensional labeled data structure with columns of potentially different types.\n",
    "\n",
    "* **pyarrow**: A Python package that provides a Python interface to the Arrow C++ library for working with columnar data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Install pyarrow**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import parquet from pyarrow as pa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Read parquet data using pyarrow.parquet (pa)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "ForecastSiteCode: int64\n",
      "ObservationTime: int64\n",
      "ObservationDate: timestamp[ms]\n",
      "WindDirection: int64\n",
      "WindSpeed: int64\n",
      "WindGust: double\n",
      "Visibility: double\n",
      "ScreenTemperature: double\n",
      "Pressure: double\n",
      "SignificantWeatherCode: int64\n",
      "SiteName: string\n",
      "Latitude: double\n",
      "Longitude: double\n",
      "Region: string\n",
      "Country: string\n",
      "----\n",
      "ForecastSiteCode: [[3002,3005,3008,3017,3023,...,3882,3002,3005,3008,3017],[3023,3026,3031,3034,3037,...,3797,3866,3872,3876,3882]]\n",
      "ObservationTime: [[0,0,0,0,0,...,12,13,13,13,13],[13,13,13,13,13,...,23,23,23,23,23]]\n",
      "ObservationDate: [[2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,2016-02-01 00:00:00.000,...,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000],[2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,2016-03-12 00:00:00.000,...,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000,2016-03-31 00:00:00.000]]\n",
      "WindDirection: [[12,10,8,6,10,...,4,8,8,8,8],[9,9,10,8,8,...,1,0,1,1,1]]\n",
      "WindSpeed: [[8,2,6,8,30,...,5,19,18,19,19],[25,26,24,23,23,...,5,10,2,3,2]]\n",
      "WindGust: [[null,null,null,null,37,...,null,null,null,null,29],[36,41,37,34,37,...,null,null,null,null,null]]\n",
      "Visibility: [[30000,35000,50000,40000,2600,...,4000,8000,3500,11000,28000],[4600,9000,30000,10000,2900,...,22000,null,50000,null,35000]]\n",
      "ScreenTemperature: [[2.1,0.1,2.8,1.6,9.8,...,10,-99,7.4,8.1,9.2],[9.1,9.5,10.2,9.7,9.9,...,4.9,8.4,3.5,6.1,3.7]]\n",
      "Pressure: [[997,997,997,996,991,...,1030,null,1019,1020,1019],[1019,1018,1020,1020,1021,...,1019,1018,1019,1019,1019]]\n",
      "SignificantWeatherCode: [[8,7,-99,8,11,...,1,5,15,12,7],[15,12,12,12,15,...,0,-99,0,-99,0]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "# file path\n",
    "parquet_path = './weather.2016.parquet'\n",
    "\n",
    "# read\n",
    "table = pa.read_table(parquet_path)\n",
    "\n",
    "# print\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Dataset shape**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194697, 15)\n"
     ]
    }
   ],
   "source": [
    "# get the dataset shape\n",
    "table_shape = table.shape\n",
    "\n",
    "# print\n",
    "print(table_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convert the pyarrow table dataset into a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ForecastSiteCode  ObservationTime ObservationDate\n",
      "0                   3002                0      2016-02-01\n",
      "1                   3005                0      2016-02-01\n",
      "2                   3008                0      2016-02-01\n",
      "3                   3017                0      2016-02-01\n",
      "4                   3023                0      2016-02-01\n",
      "...                  ...              ...             ...\n",
      "194692              3797               23      2016-03-31\n",
      "194693              3866               23      2016-03-31\n",
      "194694              3872               23      2016-03-31\n",
      "194695              3876               23      2016-03-31\n",
      "194696              3882               23      2016-03-31\n",
      "\n",
      "[194697 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# coonvert to pandas dataframe\n",
    "df = table.to_pandas()\n",
    "\n",
    "# let's take only the 3 first columns\n",
    "df = df.iloc[:, :3]\n",
    "\n",
    "# print\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
